#!/usr/bin/python
# -*- coding: UTF-8 -*-

from typing import NamedTuple

import numpy as np
from numpy.fft import fft, fftn, ifftn
from scipy.sparse.linalg import LinearOperator, minres

from .util import chinv, ndouter, outerslice, sdiv, sexp, slog, ssum


def coordinate_descent(μ,v,
    meanupdate,
    varupdate,
    maxiter     = 10,
    maxmeaniter = 10,
    maxvariter  = 1,
    tol         = 1e-5,
    alpha       = 1.0,
    verbose     = False):
    say = print if verbose else lambda *a:None
    for _j in range(maxiter):
        for i in range(maxmeaniter):
            dμ = meanupdate(μ,v)
            εμ = np.max(abs(dμ))
            μ  = μ + dμ
            say('μ iter %d, ε=%0.4e'%(i,εμ))
            if εμ<tol: break
        for i in range(maxvariter):
            dv = varupdate(μ,v)
            εv = np.max(abs(dv))
            v  = v + alpha*dv
            say('v iter %d, ε=%0.4e'%(i,εv))
            if εv<tol: break
        if εμ<tol and εv<tol: break
    return μ,v


def RI(x,rtype=np.float32):
    return np.float32(np.real(x)+np.imag(x))


def get_hartley_components(mask):
    h = mask.shape
    D = len(h)
    # Construct Fourier components as Π over 1D
    s = {*np.arange(D)}
    U = [np.any(mask,tuple([*(s-{i})])) for i in range(D)]
    # FFT each ROW
    ff = [fft(np.eye(L),axis=0,norm='ortho')[u] for L,u in zip(h,U)]
    # Index lookup for kept components in each dimension
    fi = [np.cumsum(u)-1 for u in U]
    sl = [outerslice(D,d) for d in range(D)]
    rr = []
    for ii in np.int32(np.where(mask)).T:
        ii = [ix[i] for i,ix in zip(ii,fi)]
        fc = [fj[i] for i,fj in zip(ii,ff)]
        r = np.array(fc[0],copy=True)[sl[0]]
        for d in range(1,D):
            r = r*fc[d][sl[d]]
        rr.append(r)
    rr = RI(rr)
    sh = rr.shape
    return rr.reshape(sh[0],int(np.prod(sh[1:])))
    

def lgcpnd(kf,N,K,z0f,zh0,vh0,eps=1e-5,mintol=1e-5,maxcomponents=1000,**opts):
    '''
    Args:
        kf (ndarray): ND kernel Fourier transform, unused 
            components set to zero.
        N (ndarray): ND array of binned visit counts
        K (ndarray): ND binned total spike countss
        z0f (ndarray): prior log-rate mean
        zh0: Initial guess for posterior mean log-rate
        vh0: Initial guess for posterior log-rate marginal variance
        eps (float, default 1e-3): Minimum prior eigenvalue 
            (improves numerical conditioning)
        mintol (float, default 1e-5): Tolerance for MINRES
        **opts (dict): Options for ``coordinate_descent()``
    Returns:
        tuple: (InferResult,model)
    '''
    Y = sdiv(K,N)   # Empirical rate (pseudopoint rates)
    SHAPE = N.shape
    nmask = N>0     # Mask where observations exist
    kept  = kf>0    # Mask for retained Fourier components
    if np.sum(kept)>maxcomponents: raise ValueError((
        '%d nonzero kernel Fourier components; This is a '
        'lot. Ensure that excluded components are zeroed-'
        'out in the provided kernel Fourier transform')%np.sum(kept))
    def unmask(u,mask):
        x = np.zeros(mask.shape,'f')
        x[mask] = np.ravel(u)
        return x
    def maskin(u,mask):
        return u.reshape(mask.shape)[mask]
    ten = lambda x:np.float32(x).reshape(SHAPE)
    So  = lambda u:ten(u)[nmask] #full→masked
    St  = lambda u:unmask(u,nmask) #masked→full
    n   = np.ravel(N) # visits as a vector
    y   = np.ravel(Y) # rates as a vector
    ny  = n*y      # same as np.ravel(K)
    nm  = So(n)    # nonzero visit counts
    ym  = So(y)    # rates at bins with nonzero visits
    nym = So(ny)   # total counts at bins with nonzero visits
    # Operators between low-rank subspace and (masked)state
    Gt  = lambda u:np.ravel(RI(fftn(unmask(u,kept),norm='ortho'))) #loD→mask
    Go  = lambda u:np.ravel(RI(fftn(ten(u),norm='ortho')[kept])) #mask→loD
    Ft  = lambda u:np.ravel(RI(fftn(unmask(u,kept),norm='ortho')[nmask])) #loD→mask
    Fo  = lambda u:np.ravel(RI(fftn(St(u),norm='ortho')[kept])) #mask→loD
    # Masked prior log-rate, log-mean guess
    zhf = np.ravel(np.float32(zh0)) # Initial Δ<ln(λ)> from the prior
    uh  = np.ravel(Go(zhf))      # Δ<ln(λ)> in low-rank space
    vhf = np.ravel(np.float32(vh0)) # Initial log-rate marginal variances, full spatial domain
    vh  = So(vhf)             # log-rate marginal variances for bins with data
    z0  = So(z0f)             # prior mean-log-rate in spatial domain bins with data
    # Low-rank kernel, inverse, truncated Fourier components
    kx = ifftn(kf,norm='ortho').real
    Kh = np.ravel(np.maximum(Go(kx),eps))
    Λh = 1/Kh
    Gm = get_hartley_components(kept)
    Fm = Gm[:,nmask.ravel()]
    # Preconditioner and constant terms in loss
    R  = np.size(Kh)
    Mu = lambda u:Kh*np.ravel(u)
    M  = LinearOperator((R,R),Mu,Mu,dtype=np.float32)
    ldΣz = -ssum(slog(Λh)) # ln|Σ₀|
    ll0  = .5*(ldΣz-R)
    def _nr(uh,vh):
        return nm*sexp(Ft(uh) + z0 + vh*.5)
    def loss(uh,vh):
        z    = Ft(uh) + z0
        r    = sexp(z + vh*.5)
        nyr  =  nm@(r-ym*z)         #  n'(λ-y∘μ)
        uΛu  =  ssum(uh**2*Λh)      #  μ'Λ₀μ
        C    = _C(uh,vh)
        trΛΣ =  ssum(C**2*Λh)       #  tr[Λ₀Σ]
        ldΣq =  ssum(slog(np.diag(C))) # -ln|Σ|
        return ll0 + nyr + .5*(uΛu + trΛΣ) - ldΣq
    def meanupdate(uh,vh):
        nr = _nr(uh,vh)
        J  = Λh*uh + Fo(nr-nym)
        Hu = lambda u: Λh*u + Fo(nr*(Ft(u)))
        Hv = LinearOperator((R,R),Hu,Hu,dtype=np.float32)
        return -np.float32(minres(Hv,J,rtol=mintol,M=M)[0])
    def _C(uh,vh): # Cholesky factor of covariance
        x = np.sqrt(_nr(uh,vh))[None,:]*Fm
        return chinv(np.diag(Λh) + x@x.T)
    def varupdate(uh,vh):
        return np.sum((Fm.T@_C(uh,vh))**2,1,'f')-vh
    def unpack(uh,vh):
        '''Unpack low-d mean, sparse marginal variance.'''
        x = np.sqrt(_nr(uh,vh),dtype='f')[None,:]*Fm
        C = chinv(np.diag(Λh) + x@x.T)
        v = np.sum((Gm.T@C)**2,1,'f') # Full posterior log-rate variance
        z = Gt(uh)        # full Δ mean-log-rate 
        μ = z + z0f       # full posterior mean-log-rate
        r = sexp(μ + v/2) # full posterior mean rate
        return ten(z),ten(r),ten(v),ten(μ)
    def sample(nsamples,uho=None,vho=None):
        nonlocal uh,vh
        if uho is None: uho = uh
        if vho is None: vho = vh
        du =_C(uho,vho)@np.float32(np.random.randn(R,nsamples))
        u  = uho[:,None] + du
        z  = np.zeros(SHAPE+(nsamples,))
        z[kept,:] = u
        z = RI(fftn(z,norm='ortho',axes=np.arange(len(SHAPE))))
        return z + ten(z0f)[...,None]
    uh = np.ravel(Go(zhf))
    vh = So(vhf)
    uh,vh = coordinate_descent(uh,vh,meanupdate,varupdate,**opts)
    z,r,v,μ = unpack(uh,vh) 
    # Full Δmean-log-rate and variances may be recycled as initial conditions
    state = InferState(z,v) 
    # Other useful information: 
    # r: full posterior mean rate
    # uh: low-rank Δ mean-log-rate
    info  = InferInfo(r,uh,vh,μ) 
    return InferResult(state, -loss(uh,vh), info), ClosureObject(locals())


class InferState(NamedTuple):
    '''full posterior log-rate'''
    z:np.ndarray
    '''full posterior log-variance'''
    v:np.ndarray
class InferInfo(NamedTuple):
    '''posterior mean rate'''
    r:np.ndarray
    '''low-rank Δ posterior mean from prior mean (log rate)'''
    uh:np.ndarray
    '''Posterior log-rate variance (where observations exist)'''
    vh:np.ndarray
    '''Full posterior mean-log-rate in spatial domain'''
    mu:np.ndarray
class InferResult(NamedTuple):
    '''Full log-rate and marginal log-variance'''
    zv:InferState
    '''Log-likelihood'''
    ll:float
    '''Additional information not used by grid_search'''
    info:InferInfo
class ClosureObject(dict):
    def __getattr__(self, name):
        return self[name]
    

def lgcp2d(kf,N,K,prior_mean,initial_guess=(None,None),**opts):
    '''
    Args:
        kf (ndarray): 2D kernel Fourier transform, 
            unused components set to zero.
        N (ndarray): 2D array of binned visit counts
        K (ndarray): 2D binned total spike countss
        prior_mean (ndarray): prior log-rate mean
        initial_guess (tuple, optional): Tuple of the initial 
            posterior log-mean and posterior log-marginal-variances
        **opts (dict): Keword arguments for ``lgcpnd()``
    Returns:
        tuple: (InferResult,model)
    '''
    N,K,kf = map(np.float32,(N,K,kf))
    assert N.shape==K.shape==kf.shape==prior_mean.shape
    # Priors z0, initial guess zh, masked & low-rank counterparts
    z0f = np.ravel(np.float32(prior_mean))
    # initial log-mean and marginal variance guesses
    zh0,vh0 = (None,None) if initial_guess is None else initial_guess
    if zh0 is None: zh0 = np.zeros(np.size(N),'f')
    if vh0 is None: vh0 = np.zeros(np.size(N),'f')
    assert np.size(zh0)==np.size(vh0)==np.size(N)
    return lgcpnd(kf,N,K,z0f,zh0,vh0,**opts)


def lgcpheading(kf,N,K,prior_mean,initial_guess=(None,None),**opts):
    '''
    Args:
        kf (ndarray): 3D kernel Fourier transform, 
            unused components set to zero.
        N (ndarray): heading×y×x 3D array of binned visit counts
        K (ndarray): heading×y×x 3D binned total spike countss
        prior_mean (ndarray): prior log-rate mean
        initial_guess (tuple, optional): Tuple of the initial 
            posterior log-mean and posterior log-marginal-variances
        **opts (dict): Keword arguments for ``lgcpnd()``
    Returns:
        tuple: (InferResult,model)
    '''
    N,K,kf = map(np.float32,(N,K,kf))
    assert N.shape==K.shape==kf.shape
    D,H,W = N.shape
    # Priors z0 and initial guesses zh, and their masked and low-rank counterparts
    h0 = np.ones(D,'f') # No prior opinion on heading tuning
    # prior log-rate
    if   np.size(prior_mean)==H*W:   z0f = np.ravel(ndouter(h0,prior_mean))
    elif np.size(prior_mean)==D*H*W: z0f = np.ravel(np.float32(prior_mean))
    else: assert 0
    # initial log-mean and marginal variance guesses
    zh0,vh0 = (None,None) if initial_guess is None else initial_guess
    if zh0 is None:        zhf = np.zeros(N.shape,'f')
    elif np.size(zh0)==H*W:   zhf = np.ravel(ndouter(h0,zh0))
    elif np.size(zh0)==D*H*W: zhf = np.ravel(np.float32(zh0))
    else: assert 0
    if vh0 is None:        vhf = np.zeros(N.shape,'f')
    elif np.size(vh0)==H*W:   vhf = np.ravel(ndouter(h0,vh0))
    elif np.size(vh0)==D*H*W: vhf = np.ravel(np.float32(vh0))
    else: assert 0
    return lgcpnd(kf,N,K,z0f,zhf,vhf,**opts)








